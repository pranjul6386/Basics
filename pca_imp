Before plotting the words, you need to first be able to reduce each word vector with PCA into 2 dimensions and then plot it. The steps to compute PCA are as follows:

Mean normalize the data
Compute the covariance matrix of your data ( Î£ ).
Compute the eigenvectors and the eigenvalues of your covariance matrix
Multiply the first K eigenvectors by your normalized data. The transformation should look something as follows:

######################################################################description######################################
You will write a program that takes in a data set where each row corresponds to a word vector.

The word vectors are of dimension 300.
Use PCA to change the 300 dimensions to n_components dimensions.
The new matrix should be of dimension m, n_componentns.
First de-mean the data
Get the eigenvalues using linalg.eigh. Use eigh rather than eig since R is symmetric. The performance gain when using eigh instead of eig is substantial.
Sort the eigenvectors and eigenvalues by decreasing order of the eigenvalues.
Get a subset of the eigenvectors (choose how many principle components you want to use using n_components).
Return the new transformation of the data by multiplying the eigenvectors with the original data.



#################################################################helper fxns::##################################################


Use numpy.mean(a,axis=None) : If you set axis = 0, you take the mean for each column. If you set axis = 1, you take the mean for each row. Remember that each row is a word vector, and the number of columns are the number of dimensions in a word vector.
Use numpy.cov(m, rowvar=True) . This calculates the covariance matrix. By default rowvar is True. From the documentation: "If rowvar is True (default), then each row represents a variable, with observations in the columns." In our case, each row is a word vector observation, and each column is a feature (variable).
Use numpy.linalg.eigh(a, UPLO='L')
Use numpy.argsort sorts the values in an array from smallest to largest, then returns the indices from this sort.
In order to reverse the order of a list, you can use: x[::=1].
To apply the sorted indices to eigenvalues, you can use this format x[indices_sorted].
When applying the sorted indices to eigen vectors, note that each column represents an eigenvector. In order to preserve the rows but sort on the columns, you can use this format x[:,indices_sorted]
To transform the data using a subset of the most relevant principle components, take the matrix multiplication of the eigenvectors with the original data.
The data is of shape (n_observations, n_features).
The subset of eigenvectors are in a matrix of shape (n_features, n_components).
To multiply these together, take the transposes of both the eigenvectors (n_components, n_features) and the data (n_features, n_observations).
The product of these two has dimensions (n_components,n_observations). Take its transpose to get the shape (n_observations, n_components).


################################################code##############################################################

def compute_pca(X, n_components=2):
    """
    Input: 
        X: of dimension (m,n) where each row corresponds to a word vector
        n_components: Number of components you want to keep.
    Output: 
        X_reduced: data transformed in 2 dims/columns + regenerated original data
    pass in: data as 2D NumPy array
    """
    
    
    # mean center the data
    X_demeaned = X - np.mean(X.T, axis=1)
    
    # calculate the covariance matrix
    covariance_matrix = np.cov(X_demeaned.T, rowvar=True)
    
    # calculate eigenvectors & eigenvalues of the covariance matrix
    eigen_vals, eigen_vecs = np.linalg.eigh(covariance_matrix)
    
    # sort eigenvalue in decreasing order (get the indices from the sort)
    idx_sorted = np.argsort(eigen_vals)
    
    # reverse the order so that it's from highest to lowest.
    idx_sorted_decreasing = idx_sorted[::-1]

    # sort the eigen values by idx_sorted_decreasing
    eigen_vals_sorted = eigen_vals[idx_sorted_decreasing]

    # sort eigenvectors using the idx_sorted_decreasing indices
    eigen_vecs_sorted = eigen_vecs[:, idx_sorted_decreasing]
    
    # select the first n eigenvectors (n is desired dimension
    # of rescaled data array, or dims_rescaled_data)
    eigen_vecs_subset = eigen_vecs_sorted[:, :n_components]
    
    # transform the data by multiplying the transpose of the eigenvectors#
    # with the transpose of the de-meaned data 
    # Then take the transpose of that product.
    X_reduced = np.dot(eigen_vecs_subset.T, X_demeaned.T).T
    
    ### END CODE HERE ###
    
    return X_reduced
